# Phases of Compilation

Character Stream  
⬇️  
1. **__Scanner (lexical analysis)__**  
⬇️  
Token stream  
⬇️  
2. **__Parser (syntax analysis)__**  
⬇️  
Parse tree  
⬇️  
3. **__Semantic analysis and intermediate code generation__**  
⬇️   
Abstract syntax tree or other intermediate form  
⬇️  
4. **__Machine-independent code improvement__**  
⬇️  
Modified intermediate form  
⬇️  
5. **__Target code generation__**  
⬇️  
Target language (e.g., assembler)  
⬇️  
6. **__Machine-specific code improvement__**  
⬇️  
Modified target language  

## Front end
1. Lexical analysis
2. Syntax analysis
3. Semantic analysis

## Back end
5. Target code generation
6. Machine-specific code improvement (optional)

# Program Analysis
A compiler's *Front End* is concerned with the Analysis of the source program.  
- Determining the Meaning of the source program.

Generally though of as breaking apart into three phases:  
- Lexical Analysis
- Syntactic Analysis
- Semantic Analysis

**Lexical Analysis**
Convert a text source into *tokens*.

**Syntactic Analysis**  
Convert a token stream into a *parse tree*.

**Semantic Analysis**  
Enforce semantic rules and convert a parse tree into an *intermediate form*.  
* Two activities, but often happen in an interleaved fashion.

# Syntactic Analysis  
Take a stream of *tokens* and convert it into a *parse tree*.  

Capture the *hierarchical* structure of the program.  
- Declarations, definitions, blocks, statements, expressions, ...

Provide representation for subsequent *semantic* analysis.  

Detect *syntactic* errors.  
- Mostly, improper structure, including misspelled keywords, bad statement and expression construction.
- But *not*, e.g., undeclared identifiers, mismatched function calls.
- (Why not?)

# Syntactic Analysis  
So how to do this analysis?  
In the Lexical Analysis phase, we used a formal specification of the token formats.  
- Regular expressions (with action routines).

Can we use Regular Expressions as the notation for the formal specification of program structure?  
*No!*

# Why Not Regular Expressions?  
We have shown that Regular Expressions can be directly converted to an NFA, which can then be converted to a DFA.  

And that DFA can then be used to accept or reject an input string as belonging or not belonging to the RE's language.  

The important letter here is F, for *finite*.  

That means that the DFA cannot be used to recognize, e.g., *nesting* that is "too deep".  
- There just won't be enough states in the DFA.

# Context-Free Grammar (CFG)  
Regular Expressions are limited in expressiveness.  

Cannot define strings that are required to have ...
- Well-formed and/or nested parentheses, brackets
- Matching pairs

Well-formed parentheses  
e.g., ([[{(){}[][]}]{[]}])  

Matching Pairs  
e.g., anbn ⟶ ab, aabb, aaabbb, ...  

Next step up in expressiveness is a *Context-Free Grammar*   
- Definitions can refer to themselves, i.e, can recurse.

# Context-Free Grammar  
1. A set of *Terminal Symbols*
2. A set of *Non-terminal Symbols*
3. A set of *Production Rules*
4. A *Start Symbol*

**Terminal Symbols**  
The tokens generated by the lexical analyzer  

**Non-Terminal Symbols**  
Representing syntactic categories  
Distinguished from the Terminal Symbols  

**Production Rules**  
Relating the syntactic categories to their structure.  
The LHS of each must be a single *Non-Terminal Symbol*. the RHS of each can be arbitrarily complex.  

**Start Symbol**  
One of the *Non-terminal Symbols*

# CFG Example  
expr ⟶ id | number | - expr | ( expr ) | expr op expr  
id ⟶ (\_|a|b|...|z)(_|a|b|...|z|0|1|...|9)*  
op ⟶ + | - | * | /

Notice that expr refers to itself. This definition is recursive. 
It’s not left or right recursive exclusively.
Q: What set of strings does id define?
